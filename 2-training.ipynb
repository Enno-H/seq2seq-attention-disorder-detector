{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob, os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import os\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, GRU, Concatenate, Permute, Dot, Multiply\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from nmt_utils import *\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/ennoh/Desktop/Detector/data/'\n",
    "\n",
    "raw_training = working_dir+'ALLREPORTS/'\n",
    "labels_training = working_dir+'ShAReCLEFeHealthAA_1stRoundRS_CLEFPIPEDELIMITED_200_CUIs/'\n",
    "raw_testing =  working_dir+'ALLREPORTS2/'\n",
    "#labels_testing =working_dir+'Task2ReferenceStd_CLEFShARe2013Test_StrictAndLenientpipe/'\n",
    "labels_testing =working_dir+ 'Task2ReferenceStd_CLEFShARe2013Test_TestSpansOnlypipe/'\n",
    "\n",
    "\n",
    "output = working_dir+'output_2/'\n",
    "train_bio = output+\"train_bio/\"\n",
    "test_bio = output+\"test_bio/\"\n",
    "train_tags_dict = output+'train_tags_dict'\n",
    "test_tags_dict = output+'test_tags_dict'\n",
    "\n",
    "train_doc_file = output+'train_docs.pkl'\n",
    "train_lable_file = output+'train_labels.pkl'\n",
    "train_lable_index_file = output+'train_label_index.pkl'\n",
    "\n",
    "test_doc_file = output+'test_docs.pkl'\n",
    "test_lable_file = output+'test_labels.pkl'\n",
    "test_lable_index_file = output+'test_label_index.pkl'\n",
    "\n",
    "label_dict_file = output+'label_dict.pickle'\n",
    "\n",
    "SLIDING_WINDOW = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(l, size=7):\n",
    "    \"\"\"\n",
    "    Wraps up the input string.\n",
    "    \"\"\"\n",
    "    l = list(l)\n",
    "    #左右一起补(size/2)个0\n",
    "    lpadded = size // 2 * [0] + l + size // 2 * [0]\n",
    "    \n",
    "    out = [lpadded[i:(i + size)] for i in range(len(l))]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in both train and test files\n",
    "\n",
    "#Training\n",
    "fo = open(train_doc_file,'rb')\n",
    "training_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_index_file,'rb')\n",
    "training_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_file,'rb')\n",
    "training_label_instances = pickle.load(fo)\n",
    "\n",
    "#Testing\n",
    "fo = open(test_doc_file,'rb')\n",
    "test_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_index_file,'rb')\n",
    "test_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_file,'rb')\n",
    "test_label_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(label_dict_file,'rb')\n",
    "label_dict = pickle.load(fo)\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training examples into a single file\n",
    "X_trn=[]\n",
    "for s in training_file_instances:\n",
    "    X_trn += context(s,size=SLIDING_WINDOW)\n",
    "X_trn = np.array(X_trn)\n",
    "\n",
    "y_trn=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn += list(s)\n",
    "y_trn = np.array(y_trn)\n",
    "\n",
    "#Convert testing examples into a single file\n",
    "X_tst=[]\n",
    "for s in test_file_instances:\n",
    "    X_tst += context(s,size=SLIDING_WINDOW)\n",
    "X_tst = np.array(X_tst)\n",
    "\n",
    "y_tst=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst += list(s)\n",
    "y_tst = np.array(y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_trn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_seq=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_trn_seq = np.array(y_trn_seq)\n",
    "\n",
    "y_tst_seq=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_tst_seq = np.array(y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '17830', 'llll'],\n",
       "       ['0', '17830', 'llll', '23590'],\n",
       "       ['17830', 'llll', '23590', 'llll'],\n",
       "       ...,\n",
       "       ['end', 'of', 'report', '¤'],\n",
       "       ['of', 'report', '¤', '¤'],\n",
       "       ['report', '¤', '¤', '0']], dtype='<U30')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 4\n",
    "train_texts = [' '.join(text) for text in X_trn]\n",
    "test_texts = [' '.join(text) for text in X_tst]\n",
    "\n",
    "all_texts = train_texts + test_texts\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(all_texts)\n",
    "word_to_index = t.word_index\n",
    "vocab_len = len(word_to_index) + 1\n",
    "encoded_X_train = t.texts_to_sequences(train_texts)\n",
    "encoded_X_test = t.texts_to_sequences(test_texts)\n",
    "\n",
    "#padding\n",
    "encoded_X_train = pad_sequences(encoded_X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "encoded_X_test = pad_sequences(encoded_X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   21,    21, 10766,     8],\n",
       "       [   21, 10766,     8,  6143],\n",
       "       [10766,     8,  6143,     8],\n",
       "       ...,\n",
       "       [  200,     6,    70,     1],\n",
       "       [    6,    70,     1,     1],\n",
       "       [   70,     1,     1,    21]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape: (115916, 4)\n",
      "y_trn.shape: (115916,)\n",
      "y_trn_seq.shape: (115916, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trn.shape:\", X_trn.shape)\n",
    "print(\"y_trn.shape:\", y_trn.shape)\n",
    "print(\"y_trn_seq.shape:\", y_trn_seq.shape)\n",
    "\n",
    "#print(\"Xoh.shape:\", Xoh.shape)\n",
    "#print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: ['04' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 97 252 361   1]\n",
      "\n",
      "Target date: 1\n",
      "Target date (seq): [0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source date:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target date:\", y_trn[index])\n",
    "print(\"Target date (seq):\", y_trn_seq[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "#print(\"Target after preprocessing (indices):\", encoded_X_test[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "#print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1,2,3]\n",
    "convert_to_one_hot(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [0,0,1,0]\n",
    "convert_to_one_hot(b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoh_train = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_trn_seq)))\n",
    "Yoh_test = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_tst_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115916, 4, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108856, 4, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['0' '0' '17830' 'llll']\n",
      "Source after preprocessing (indices): [   21    21 10766     8]\n",
      "\n",
      "Target data: 0\n",
      "Target data (seq): [0 0 0 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source data:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target data:\", y_trn[index])\n",
    "print(\"Target data (seq):\", y_trn_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['20' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 99 252 361   1]\n",
      "\n",
      "Target data: 1\n",
      "Target data (seq): [0 0 1 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source data:\", X_tst[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_test[index])\n",
    "print()\n",
    "print(\"Target data:\", y_tst[index])\n",
    "print(\"Target data (seq):\", y_tst_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_test[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model = FastText.load_fasttext_format('mimic_3_fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    #emb_dim = 200\n",
    "    #len(word_to_index) = 11053\n",
    "    #vocab_len = 11054\n",
    "    \n",
    "    emb_dim = mimic_model[\"ca\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in mimic_model.wv.vocab:\n",
    "            emb_matrix[index, :] = mimic_model[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "            \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21576422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_weights()[0][3][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "Tx = 4\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "#output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "output_layer = Dense(2, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"      embedding_dim\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"      label_number\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    \n",
    "    sentence_indices = Input(shape=(Tx,), dtype=np.int32)\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    #X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "        \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([sentence_indices, s0, c0], outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(4, 4, n_a, n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4, 200)       2211000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 4, 128)       135680      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 4, 128)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 256)       0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4, 1)         257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 4, 1)         0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,478,779\n",
      "Trainable params: 2,478,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "115916/115916 [==============================] - 231s 2ms/step - loss: 0.1455 - dense_2_loss: 0.0351 - dense_2_acc: 0.9878 - dense_2_acc_1: 0.9849 - dense_2_acc_2: 0.9860 - dense_2_acc_3: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a500cca90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoded_X_train, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 's0:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'c0:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_1/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('model/model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  28  101    3 2014]\n",
      " [  28   15 2114 3347]\n",
      " [  97  252  361    1]\n",
      " [ 101    3 2014  119]\n",
      " [   3   85   15   84]\n",
      " [  85   15   84  283]]\n",
      "(4, 6, 2)\n",
      "(6, 4, 2)\n",
      "(6, 4)\n",
      "Prediction: \n",
      "[[0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['she has the ca', 'she is working hard', '04 sex f ¤', 'has the ca but', 'the pt is not', 'pt is not good']\n",
    "\n",
    "EXAMPLES = t.texts_to_sequences(EXAMPLES)\n",
    "EXAMPLES = pad_sequences(EXAMPLES, maxlen=4)\n",
    "\n",
    "print(EXAMPLES)\n",
    "PREDICTION = model.predict([EXAMPLES, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "print(PREDICTION.shape) #(4, 3, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "print(PREDICTION.shape) #(3, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "\n",
    "#print('User Input: ' , E)\n",
    "print('Prediction: ')\n",
    "print(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 108856, 2)\n",
      "(108856, 4, 2)\n",
      "(108856, 4)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_ = model.predict([encoded_X_test, s0, c0])\n",
    "\n",
    "#print(PREDICTION_)\n",
    "PREDICTION_ = np.asarray(PREDICTION_)\n",
    "print(PREDICTION_.shape) #(4, 108856, 2)\n",
    "PREDICTION_ = PREDICTION_.swapaxes(0,1)\n",
    "print(PREDICTION_.shape) #(108856, 4, 2)\n",
    "\n",
    "PR_ = np.argmax(PREDICTION_, axis = -1)\n",
    "print(PR_.shape)\n",
    "\n",
    "print(PR_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   21,    21, 10963,     8],\n",
       "       [   21, 10963,     8,  8528],\n",
       "       [10963,     8,  8528,     8],\n",
       "       ...,\n",
       "       [  200,     6,    70,     1],\n",
       "       [    6,    70,     1,     1],\n",
       "       [   70,     1,     1,    21]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([[0,0,0,0],[0,0,0,1]])\n",
    "b = np.asarray([[0,0,0,0],[0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=a,y_pred=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955206878812376"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(PR_,y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on document Plan_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('task2_train_dict','rb')\n",
    "task2_test_dict = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ALLREPORTS2/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(filename, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_index_list = list(task2_test_dict[filename.split('/')[-1]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "last = 0\n",
    "for var in abbr_index_list:\n",
    "    begin = int(var[0])\n",
    "    end = int(var[1])\n",
    "    output = output + document[last:begin] + \"@@@\"\n",
    "    last = end\n",
    "output = output + document[last:len(document)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on '00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/output_2/test_bio/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'O': 0, 'Disorder': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(filename)\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in lines:\n",
    "    text.append(line.split('\\t')[0])\n",
    "    label.append(line.split('\\t')[-1].replace('\\n',''))        \n",
    "new_label = []\n",
    "for var in label:       \n",
    "    new_label.append(label_dict[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_groups(init_list, children_list_len):\n",
    "    list_of_groups = zip(*(iter(init_list),) *children_list_len)\n",
    "    end_list = [list(i) for i in list_of_groups]\n",
    "    count = len(init_list) % children_list_len\n",
    "    end_list.append(init_list[-count:]) if count !=0 else end_list\n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_4 = list_of_groups(text,4)\n",
    "new_text_by_4 = [' '.join(i) for i in text_by_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 4)\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "X = t.texts_to_sequences(new_text_by_4)\n",
    "X = pad_sequences(X, maxlen=4)\n",
    "\n",
    "#print(X)\n",
    "PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "#print(PREDICTION.shape) #(4, m, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "#print(PREDICTION.shape) #(m, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "print(len(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneline_prediction = PR.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneline_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750000000000001"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(new_label,oneline_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on 99 reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_single_file(filename):\n",
    "    text = []\n",
    "    label = []\n",
    "    new_label = []\n",
    "    f = open(filename)\n",
    "    for line in f.readlines():\n",
    "        text.append(line.split('\\t')[0])\n",
    "        label.append(line.split('\\t')[-1].replace('\\n',''))        \n",
    "    for var in label:       \n",
    "        new_label.append(label_dict[var])\n",
    "    text_by_4 = list_of_groups(text,4)\n",
    "    new_text_by_4 = [' '.join(i) for i in text_by_4]\n",
    "    \n",
    "    X = t.texts_to_sequences(new_text_by_4)\n",
    "    X = pad_sequences(X, maxlen=4)\n",
    "\n",
    "    PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "    PREDICTION = np.asarray(PREDICTION)\n",
    "    PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "\n",
    "    PR = np.argmax(PREDICTION, axis = -1)\n",
    "    oneline_prediction = PR.flatten()\n",
    "    oneline_prediction = oneline_prediction[:len(new_label)]\n",
    "    \n",
    "    print(filename.split('/')[-1],'  Number: ',len(new_label),'  F1:',f1_score(new_label,oneline_prediction), '  Presicion: ',precision_score(new_label,oneline_prediction), '  Recall: ',recall_score(new_label,oneline_prediction))\n",
    "    return f1_score(new_label,oneline_prediction), new_label, list(oneline_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02740-024700-DISCHARGE_SUMMARY.txt   Number:  902   F1: 0.823529411764706   Presicion:  0.7777777777777778   Recall:  0.875\n",
      "09703-109051-ECHO_REPORT.txt   Number:  453   F1: 0.9199999999999999   Presicion:  0.9583333333333334   Recall:  0.8846153846153846\n",
      "03087-026480-DISCHARGE_SUMMARY.txt   Number:  1346   F1: 0.8571428571428571   Presicion:  0.8571428571428571   Recall:  0.8571428571428571\n",
      "08990-002227-DISCHARGE_SUMMARY.txt   Number:  2283   F1: 0.9090909090909091   Presicion:  0.8974358974358975   Recall:  0.9210526315789473\n",
      "19138-025729-DISCHARGE_SUMMARY.txt   Number:  1232   F1: 0.7499999999999999   Presicion:  1.0   Recall:  0.6\n",
      "04082-167766-RADIOLOGY_REPORT.txt   Number:  302   F1: 0.7999999999999999   Presicion:  0.8235294117647058   Recall:  0.7777777777777778\n",
      "17652-018982-DISCHARGE_SUMMARY.txt   Number:  1582   F1: 0.7777777777777777   Presicion:  0.875   Recall:  0.7\n",
      "21951-203738-RADIOLOGY_REPORT.txt   Number:  267   F1: 0.8679245283018867   Presicion:  0.92   Recall:  0.8214285714285714\n",
      "16997-000825-DISCHARGE_SUMMARY.txt   Number:  1597   F1: 0.75   Presicion:  0.75   Recall:  0.75\n",
      "00176-102920-ECHO_REPORT.txt   Number:  452   F1: 0.8750000000000001   Presicion:  1.0   Recall:  0.7777777777777778\n",
      "21312-018707-DISCHARGE_SUMMARY.txt   Number:  1337   F1: 0.3076923076923077   Presicion:  0.8   Recall:  0.19047619047619047\n",
      "03298-014440-DISCHARGE_SUMMARY.txt   Number:  876   F1: 0.5263157894736842   Presicion:  0.5   Recall:  0.5555555555555556\n",
      "00534-017453-DISCHARGE_SUMMARY.txt   Number:  1438   F1: 0.6666666666666666   Presicion:  0.5   Recall:  1.0\n",
      "05382-010331-DISCHARGE_SUMMARY.txt   Number:  1832   F1: 0.7905405405405405   Presicion:  0.7697368421052632   Recall:  0.8125\n",
      "09339-028983-DISCHARGE_SUMMARY.txt   Number:  1192   F1: 0.6923076923076924   Presicion:  0.75   Recall:  0.6428571428571429\n",
      "05065-011493-DISCHARGE_SUMMARY.txt   Number:  2394   F1: 0.825   Presicion:  0.8181818181818182   Recall:  0.8319327731092437\n",
      "13990-101915-ECHO_REPORT.txt   Number:  379   F1: 0.9090909090909091   Presicion:  1.0   Recall:  0.8333333333333334\n",
      "11392-010791-DISCHARGE_SUMMARY.txt   Number:  1592   F1: 0.8707482993197277   Presicion:  0.8888888888888888   Recall:  0.8533333333333334\n",
      "18114-360237-RADIOLOGY_REPORT.txt   Number:  296   F1: 0.888888888888889   Presicion:  0.8   Recall:  1.0\n",
      "20389-024150-DISCHARGE_SUMMARY.txt   Number:  1737   F1: 0.8016528925619835   Presicion:  0.7950819672131147   Recall:  0.8083333333333333\n",
      "07214-025053-DISCHARGE_SUMMARY.txt   Number:  1502   F1: 0.8484848484848484   Presicion:  0.7368421052631579   Recall:  1.0\n",
      "14285-022846-DISCHARGE_SUMMARY.txt   Number:  1137   F1: 0.7577092511013216   Presicion:  0.8865979381443299   Recall:  0.6615384615384615\n",
      "09602-000963-DISCHARGE_SUMMARY.txt   Number:  945   F1: 0.4444444444444444   Presicion:  0.5714285714285714   Recall:  0.36363636363636365\n",
      "00534-100076-ECHO_REPORT.txt   Number:  344   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "16660-004075-DISCHARGE_SUMMARY.txt   Number:  1431   F1: 0.8701298701298701   Presicion:  0.881578947368421   Recall:  0.8589743589743589\n",
      "03835-028462-DISCHARGE_SUMMARY.txt   Number:  511   F1: 0.8181818181818182   Presicion:  0.8181818181818182   Recall:  0.8181818181818182\n",
      "05163-019624-DISCHARGE_SUMMARY.txt   Number:  879   F1: 0.5   Presicion:  1.0   Recall:  0.3333333333333333\n",
      "19154-166217-RADIOLOGY_REPORT.txt   Number:  210   F1: 0.8235294117647058   Presicion:  1.0   Recall:  0.7\n",
      "25775-007416-DISCHARGE_SUMMARY.txt   Number:  2568   F1: 0.8111455108359132   Presicion:  0.8291139240506329   Recall:  0.793939393939394\n",
      "12530-004020-DISCHARGE_SUMMARY.txt   Number:  1508   F1: 0.8454545454545455   Presicion:  0.9029126213592233   Recall:  0.7948717948717948\n",
      "11552-026221-DISCHARGE_SUMMARY.txt   Number:  1098   F1: 0.7333333333333334   Presicion:  0.8   Recall:  0.676923076923077\n",
      "12125-022364-DISCHARGE_SUMMARY.txt   Number:  1796   F1: 0.8775510204081632   Presicion:  0.9347826086956522   Recall:  0.8269230769230769\n",
      "05837-000274-DISCHARGE_SUMMARY.txt   Number:  1416   F1: 0.6060606060606061   Presicion:  0.5882352941176471   Recall:  0.625\n",
      "08415-016301-DISCHARGE_SUMMARY.txt   Number:  1530   F1: 0.5416666666666666   Presicion:  0.7647058823529411   Recall:  0.41935483870967744\n",
      "11681-022505-DISCHARGE_SUMMARY.txt   Number:  1610   F1: 0.7563025210084034   Presicion:  0.7627118644067796   Recall:  0.75\n",
      "16677-010128-DISCHARGE_SUMMARY.txt   Number:  370   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "17774-014129-DISCHARGE_SUMMARY.txt   Number:  1235   F1: 0.6666666666666667   Presicion:  0.5384615384615384   Recall:  0.875\n",
      "07683-016743-DISCHARGE_SUMMARY.txt   Number:  1199   F1: 0.6666666666666665   Presicion:  0.8571428571428571   Recall:  0.5454545454545454\n",
      "22788-021533-DISCHARGE_SUMMARY.txt   Number:  1471   F1: 0.7755102040816326   Presicion:  0.7916666666666666   Recall:  0.76\n",
      "16743-013010-DISCHARGE_SUMMARY.txt   Number:  364   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "15230-012950-DISCHARGE_SUMMARY.txt   Number:  1136   F1: 0.8749999999999999   Presicion:  0.8235294117647058   Recall:  0.9333333333333333\n",
      "15664-014779-DISCHARGE_SUMMARY.txt   Number:  532   F1: 0.33333333333333337   Presicion:  0.2   Recall:  1.0\n",
      "18531-010240-DISCHARGE_SUMMARY.txt   Number:  1680   F1: 0.7753623188405797   Presicion:  0.7867647058823529   Recall:  0.7642857142857142\n",
      "24307-009748-DISCHARGE_SUMMARY.txt   Number:  1072   F1: 0.7647058823529412   Presicion:  0.7647058823529411   Recall:  0.7647058823529411\n",
      "12627-109059-ECHO_REPORT.txt   Number:  315   F1: 0.9444444444444444   Presicion:  1.0   Recall:  0.8947368421052632\n",
      "06557-009968-DISCHARGE_SUMMARY.txt   Number:  943   F1: 0.828125   Presicion:  0.8688524590163934   Recall:  0.7910447761194029\n",
      "16072-170823-RADIOLOGY_REPORT.txt   Number:  214   F1: 0.8484848484848485   Presicion:  0.8235294117647058   Recall:  0.875\n",
      "20223-103427-ECHO_REPORT.txt   Number:  332   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "18108-381702-RADIOLOGY_REPORT.txt   Number:  234   F1: 0.8571428571428571   Presicion:  0.8571428571428571   Recall:  0.8571428571428571\n",
      "16134-204168-RADIOLOGY_REPORT.txt   Number:  390   F1: 0.9230769230769231   Presicion:  0.9230769230769231   Recall:  0.9230769230769231\n",
      "20442-023289-DISCHARGE_SUMMARY.txt   Number:  1173   F1: 0.8461538461538461   Presicion:  0.868421052631579   Recall:  0.825\n",
      "09248-026497-DISCHARGE_SUMMARY.txt   Number:  1736   F1: 0.7957746478873239   Presicion:  0.8432835820895522   Recall:  0.7533333333333333\n",
      "11378-103592-ECHO_REPORT.txt   Number:  381   F1: 0.8   Presicion:  0.9565217391304348   Recall:  0.6875\n",
      "01222-104065-ECHO_REPORT.txt   Number:  307   F1: 0.923076923076923   Presicion:  1.0   Recall:  0.8571428571428571\n",
      "09531-108127-ECHO_REPORT.txt   Number:  342   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "04882-004677-DISCHARGE_SUMMARY.txt   Number:  1786   F1: 0.8333333333333333   Presicion:  0.8823529411764706   Recall:  0.7894736842105263\n",
      "11098-004672-DISCHARGE_SUMMARY.txt   Number:  1165   F1: 0.8666666666666666   Presicion:  0.9069767441860465   Recall:  0.8297872340425532\n",
      "24435-000622-DISCHARGE_SUMMARY.txt   Number:  469   F1: 0.8181818181818182   Presicion:  0.8181818181818182   Recall:  0.8181818181818182\n",
      "17467-010718-DISCHARGE_SUMMARY.txt   Number:  1604   F1: 0.8220858895705522   Presicion:  0.9436619718309859   Recall:  0.7282608695652174\n",
      "06134-005003-DISCHARGE_SUMMARY.txt   Number:  1636   F1: 0.8539325842696629   Presicion:  0.9421487603305785   Recall:  0.7808219178082192\n",
      "15751-026988-DISCHARGE_SUMMARY.txt   Number:  1480   F1: 0.8240343347639485   Presicion:  0.897196261682243   Recall:  0.7619047619047619\n",
      "10689-110055-ECHO_REPORT.txt   Number:  391   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "17644-017974-DISCHARGE_SUMMARY.txt   Number:  971   F1: 0.33333333333333337   Presicion:  0.2   Recall:  1.0\n",
      "26522-011368-DISCHARGE_SUMMARY.txt   Number:  1540   F1: 0.7552447552447553   Presicion:  0.8059701492537313   Recall:  0.7105263157894737\n",
      "08216-388895-RADIOLOGY_REPORT.txt   Number:  205   F1: 0.8888888888888888   Presicion:  0.8888888888888888   Recall:  0.8888888888888888\n",
      "22159-004946-DISCHARGE_SUMMARY.txt   Number:  1348   F1: 0.8471615720524017   Presicion:  0.8508771929824561   Recall:  0.8434782608695652\n",
      "19911-175533-RADIOLOGY_REPORT.txt   Number:  196   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "17583-022047-DISCHARGE_SUMMARY.txt   Number:  1309   F1: 0.8648648648648648   Presicion:  0.9411764705882353   Recall:  0.8\n",
      "17054-016976-DISCHARGE_SUMMARY.txt   Number:  1410   F1: 0.8571428571428572   Presicion:  0.9   Recall:  0.8181818181818182\n",
      "10773-027033-DISCHARGE_SUMMARY.txt   Number:  1443   F1: 0.6857142857142857   Presicion:  0.75   Recall:  0.631578947368421\n",
      "10539-022213-DISCHARGE_SUMMARY.txt   Number:  1698   F1: 0.8500000000000001   Presicion:  0.8947368421052632   Recall:  0.8095238095238095\n",
      "08786-003318-DISCHARGE_SUMMARY.txt   Number:  1905   F1: 0.7701149425287357   Presicion:  0.8072289156626506   Recall:  0.7362637362637363\n",
      "18317-007698-DISCHARGE_SUMMARY.txt   Number:  582   F1: 0.9090909090909091   Presicion:  0.9090909090909091   Recall:  0.9090909090909091\n",
      "01160-000945-DISCHARGE_SUMMARY.txt   Number:  813   F1: 0.7999999999999999   Presicion:  0.75   Recall:  0.8571428571428571\n",
      "21979-010316-DISCHARGE_SUMMARY.txt   Number:  1034   F1: 0.9213483146067417   Presicion:  0.9534883720930233   Recall:  0.8913043478260869\n",
      "15789-007213-DISCHARGE_SUMMARY.txt   Number:  954   F1: 0.7272727272727273   Presicion:  0.5714285714285714   Recall:  1.0\n",
      "24786-014472-DISCHARGE_SUMMARY.txt   Number:  1079   F1: 0.7368421052631579   Presicion:  0.6363636363636364   Recall:  0.875\n",
      "20706-009354-DISCHARGE_SUMMARY.txt   Number:  924   F1: 0.8333333333333333   Presicion:  0.7142857142857143   Recall:  1.0\n",
      "04525-003099-DISCHARGE_SUMMARY.txt   Number:  1789   F1: 0.8497854077253219   Presicion:  0.9428571428571428   Recall:  0.7734375\n",
      "03628-023268-DISCHARGE_SUMMARY.txt   Number:  1670   F1: 0.5517241379310345   Presicion:  0.42105263157894735   Recall:  0.8\n",
      "21815-002962-DISCHARGE_SUMMARY.txt   Number:  1233   F1: 0.7142857142857143   Presicion:  0.8333333333333334   Recall:  0.625\n",
      "07546-000040-DISCHARGE_SUMMARY.txt   Number:  960   F1: 0.6153846153846153   Presicion:  0.6666666666666666   Recall:  0.5714285714285714\n",
      "17097-368450-RADIOLOGY_REPORT.txt   Number:  226   F1: 0.9302325581395349   Presicion:  1.0   Recall:  0.8695652173913043\n",
      "04995-028156-DISCHARGE_SUMMARY.txt   Number:  1336   F1: 0.8333333333333333   Presicion:  0.8   Recall:  0.8695652173913043\n",
      "08324-097667-ECHO_REPORT.txt   Number:  394   F1: 1.0   Presicion:  1.0   Recall:  1.0\n",
      "16247-028319-DISCHARGE_SUMMARY.txt   Number:  1515   F1: 0.8051948051948052   Presicion:  0.8611111111111112   Recall:  0.7560975609756098\n",
      "20701-013632-DISCHARGE_SUMMARY.txt   Number:  1364   F1: 0.5384615384615384   Presicion:  0.3684210526315789   Recall:  1.0\n",
      "12582-011060-DISCHARGE_SUMMARY.txt   Number:  1766   F1: 0.8827586206896552   Presicion:  0.8533333333333334   Recall:  0.9142857142857143\n",
      "19596-007256-DISCHARGE_SUMMARY.txt   Number:  1344   F1: 0.9103448275862068   Presicion:  0.88   Recall:  0.9428571428571428\n",
      "01163-001840-DISCHARGE_SUMMARY.txt   Number:  2616   F1: 0.8079999999999999   Presicion:  0.8145161290322581   Recall:  0.8015873015873016\n",
      "25150-027400-DISCHARGE_SUMMARY.txt   Number:  1187   F1: 0.9158878504672897   Presicion:  0.9423076923076923   Recall:  0.8909090909090909\n",
      "16055-152402-RADIOLOGY_REPORT.txt   Number:  265   F1: 0.9090909090909091   Presicion:  0.9090909090909091   Recall:  0.9090909090909091\n",
      "00381-006281-DISCHARGE_SUMMARY.txt   Number:  1351   F1: 0.7999999999999999   Presicion:  0.8235294117647058   Recall:  0.7777777777777778\n",
      "15021-016750-DISCHARGE_SUMMARY.txt   Number:  1071   F1: 0.6666666666666666   Presicion:  0.5   Recall:  1.0\n",
      "12618-027862-DISCHARGE_SUMMARY.txt   Number:  1603   F1: 0.8296296296296296   Presicion:  0.8484848484848485   Recall:  0.8115942028985508\n",
      "07797-005646-DISCHARGE_SUMMARY.txt   Number:  1656   F1: 0.7839999999999999   Presicion:  0.7777777777777778   Recall:  0.7903225806451613\n",
      "21115-101632-ECHO_REPORT.txt   Number:  464   F1: 0.9259259259259259   Presicion:  1.0   Recall:  0.8620689655172413\n",
      "10434-169426-RADIOLOGY_REPORT.txt   Number:  216   F1: 0.896551724137931   Presicion:  1.0   Recall:  0.8125\n",
      "19778-001791-DISCHARGE_SUMMARY.txt   Number:  1488   F1: 0.9370629370629372   Presicion:  0.9436619718309859   Recall:  0.9305555555555556\n",
      "\n",
      "Total number of words:  108856\n",
      "Final F1:  0.8223886093040383  Final Precision:  0.8515505846466701  Final Recall:  0.7951578447661999\n"
     ]
    }
   ],
   "source": [
    "all_label = []\n",
    "all_prediction = []\n",
    "for root, dirs, files in os.walk('data/output_2/test_bio/'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file = 'data/output_2/test_bio/' + file\n",
    "            f1, label, prediction = evaluate_with_single_file(file)\n",
    "            all_label = all_label + label\n",
    "            all_prediction = all_prediction + prediction\n",
    "\n",
    "print()\n",
    "print('Total number of words: ', len(all_prediction))\n",
    "print('Final F1: ',f1_score(all_label,all_prediction), ' Final Precision: ', precision_score(all_label,all_prediction), ' Final Recall: ', recall_score(all_label,all_prediction)  )\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104059,    584],\n",
       "       [   863,   3350]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(all_label,all_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
