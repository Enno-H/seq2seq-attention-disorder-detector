{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob, os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import os\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, GRU, Concatenate, Permute, Dot, Multiply\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from nmt_utils import *\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/ennoh/Desktop/Detector/data/'\n",
    "\n",
    "raw_training = working_dir+'ALLREPORTS/'\n",
    "labels_training = working_dir+'ShAReCLEFeHealthAA_1stRoundRS_CLEFPIPEDELIMITED_200_CUIs/'\n",
    "raw_testing =  working_dir+'ALLREPORTS2/'\n",
    "#labels_testing =working_dir+'Task2ReferenceStd_CLEFShARe2013Test_StrictAndLenientpipe/'\n",
    "labels_testing =working_dir+ 'Task2ReferenceStd_CLEFShARe2013Test_TestSpansOnlypipe/'\n",
    "\n",
    "\n",
    "output = working_dir+'output_2/'\n",
    "train_bio = output+\"train_bio/\"\n",
    "test_bio = output+\"test_bio/\"\n",
    "train_tags_dict = output+'train_tags_dict'\n",
    "test_tags_dict = output+'test_tags_dict'\n",
    "\n",
    "train_doc_file = output+'train_docs.pkl'\n",
    "train_lable_file = output+'train_labels.pkl'\n",
    "train_lable_index_file = output+'train_label_index.pkl'\n",
    "\n",
    "test_doc_file = output+'test_docs.pkl'\n",
    "test_lable_file = output+'test_labels.pkl'\n",
    "test_lable_index_file = output+'test_label_index.pkl'\n",
    "\n",
    "label_dict_file = output+'label_dict.pickle'\n",
    "\n",
    "SLIDING_WINDOW = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(l, size=7):\n",
    "    \"\"\"\n",
    "    Wraps up the input string.\n",
    "    \"\"\"\n",
    "    l = list(l)\n",
    "    #左右一起补(size/2)个0\n",
    "    lpadded = size // 2 * [0] + l + size // 2 * [0]\n",
    "    \n",
    "    out = [lpadded[i:(i + size)] for i in range(len(l))]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in both train and test files\n",
    "\n",
    "#Training\n",
    "fo = open(train_doc_file,'rb')\n",
    "training_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_index_file,'rb')\n",
    "training_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_file,'rb')\n",
    "training_label_instances = pickle.load(fo)\n",
    "\n",
    "#Testing\n",
    "fo = open(test_doc_file,'rb')\n",
    "test_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_index_file,'rb')\n",
    "test_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_file,'rb')\n",
    "test_label_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(label_dict_file,'rb')\n",
    "label_dict = pickle.load(fo)\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training examples into a single file\n",
    "X_trn=[]\n",
    "for s in training_file_instances:\n",
    "    X_trn += context(s,size=SLIDING_WINDOW)\n",
    "X_trn = np.array(X_trn)\n",
    "\n",
    "y_trn=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn += list(s)\n",
    "y_trn = np.array(y_trn)\n",
    "\n",
    "#Convert testing examples into a single file\n",
    "X_tst=[]\n",
    "for s in test_file_instances:\n",
    "    X_tst += context(s,size=SLIDING_WINDOW)\n",
    "X_tst = np.array(X_tst)\n",
    "\n",
    "y_tst=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst += list(s)\n",
    "y_tst = np.array(y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_trn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_seq=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_trn_seq = np.array(y_trn_seq)\n",
    "\n",
    "y_tst_seq=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_tst_seq = np.array(y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '17830', 'llll'],\n",
       "       ['0', '17830', 'llll', '23590'],\n",
       "       ['17830', 'llll', '23590', 'llll'],\n",
       "       ...,\n",
       "       ['end', 'of', 'report', '¤'],\n",
       "       ['of', 'report', '¤', '¤'],\n",
       "       ['report', '¤', '¤', '0']], dtype='<U30')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 4\n",
    "train_texts = [' '.join(text) for text in X_trn]\n",
    "test_texts = [' '.join(text) for text in X_tst]\n",
    "\n",
    "all_texts = train_texts + test_texts\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(all_texts)\n",
    "word_to_index = t.word_index\n",
    "vocab_len = len(word_to_index) + 1\n",
    "encoded_X_train = t.texts_to_sequences(train_texts)\n",
    "encoded_X_test = t.texts_to_sequences(test_texts)\n",
    "\n",
    "#padding\n",
    "encoded_X_train = pad_sequences(encoded_X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "encoded_X_test = pad_sequences(encoded_X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   21,    21, 10766,     8],\n",
       "       [   21, 10766,     8,  6143],\n",
       "       [10766,     8,  6143,     8],\n",
       "       ...,\n",
       "       [  200,     6,    70,     1],\n",
       "       [    6,    70,     1,     1],\n",
       "       [   70,     1,     1,    21]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape: (115916, 4)\n",
      "y_trn.shape: (115916,)\n",
      "y_trn_seq.shape: (115916, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trn.shape:\", X_trn.shape)\n",
    "print(\"y_trn.shape:\", y_trn.shape)\n",
    "print(\"y_trn_seq.shape:\", y_trn_seq.shape)\n",
    "\n",
    "#print(\"Xoh.shape:\", Xoh.shape)\n",
    "#print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: ['04' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 97 252 361   1]\n",
      "\n",
      "Target date: 1\n",
      "Target date (seq): [0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source date:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target date:\", y_trn[index])\n",
    "print(\"Target date (seq):\", y_trn_seq[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "#print(\"Target after preprocessing (indices):\", encoded_X_test[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "#print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1,2,3]\n",
    "convert_to_one_hot(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [0,0,1,0]\n",
    "convert_to_one_hot(b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoh_train = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_trn_seq)))\n",
    "Yoh_test = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_tst_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115916, 4, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108856, 4, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['0' '0' '17830' 'llll']\n",
      "Source after preprocessing (indices): [   21    21 10766     8]\n",
      "\n",
      "Target data: 0\n",
      "Target data (seq): [0 0 0 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source data:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target data:\", y_trn[index])\n",
    "print(\"Target data (seq):\", y_trn_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['20' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 99 252 361   1]\n",
      "\n",
      "Target data: 1\n",
      "Target data (seq): [0 0 1 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source data:\", X_tst[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_test[index])\n",
    "print()\n",
    "print(\"Target data:\", y_tst[index])\n",
    "print(\"Target data (seq):\", y_tst_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_test[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model = FastText.load_fasttext_format('mimic_3_fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    #emb_dim = 200\n",
    "    #len(word_to_index) = 11053\n",
    "    #vocab_len = 11054\n",
    "    \n",
    "    emb_dim = mimic_model[\"ca\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in mimic_model.wv.vocab:\n",
    "            emb_matrix[index, :] = mimic_model[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "            \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21576422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_weights()[0][3][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "Tx = 4\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "#output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "output_layer = Dense(2, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"      embedding_dim\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"      label_number\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    \n",
    "    sentence_indices = Input(shape=(Tx,), dtype=np.int32)\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    #X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "        \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([sentence_indices, s0, c0], outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(4, 4, n_a, n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4, 200)       2211000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 4, 128)       135680      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 4, 128)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 256)       0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4, 1)         257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 4, 1)         0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,478,779\n",
      "Trainable params: 2,478,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "115916/115916 [==============================] - 52s 447us/step - loss: 0.1454 - dense_2_loss: 0.0350 - dense_2_acc: 0.9878 - dense_2_acc_1: 0.9849 - dense_2_acc_2: 0.9860 - dense_2_acc_3: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a57103cf8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoded_X_train, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  28  101    3 2014]\n",
      " [  28   15 2114 3347]\n",
      " [  97  252  361    1]\n",
      " [ 101    3 2014  119]\n",
      " [   3   85   15   84]\n",
      " [  85   15   84  283]]\n",
      "(4, 6, 2)\n",
      "(6, 4, 2)\n",
      "(6, 4)\n",
      "Prediction: \n",
      "[[0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['she has the ca', 'she is working hard', '04 sex f ¤', 'has the ca but', 'the pt is not', 'pt is not good']\n",
    "\n",
    "EXAMPLES = t.texts_to_sequences(EXAMPLES)\n",
    "EXAMPLES = pad_sequences(EXAMPLES, maxlen=4)\n",
    "\n",
    "print(EXAMPLES)\n",
    "PREDICTION = model.predict([EXAMPLES, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "print(PREDICTION.shape) #(4, 3, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "print(PREDICTION.shape) #(3, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "\n",
    "#print('User Input: ' , E)\n",
    "print('Prediction: ')\n",
    "print(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 108856, 2)\n",
      "(108856, 4, 2)\n",
      "(108856, 4)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_ = model.predict([encoded_X_test, s0, c0])\n",
    "\n",
    "#print(PREDICTION_)\n",
    "PREDICTION_ = np.asarray(PREDICTION_)\n",
    "print(PREDICTION_.shape) #(4, 108856, 2)\n",
    "PREDICTION_ = PREDICTION_.swapaxes(0,1)\n",
    "print(PREDICTION_.shape) #(108856, 4, 2)\n",
    "\n",
    "PR_ = np.argmax(PREDICTION_, axis = -1)\n",
    "print(PR_.shape)\n",
    "\n",
    "print(PR_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   21,    21, 10963,     8],\n",
       "       [   21, 10963,     8,  8528],\n",
       "       [10963,     8,  8528,     8],\n",
       "       ...,\n",
       "       [  200,     6,    70,     1],\n",
       "       [    6,    70,     1,     1],\n",
       "       [   70,     1,     1,    21]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([[0,0,0,0],[0,0,0,1]])\n",
    "b = np.asarray([[0,0,0,0],[0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=a,y_pred=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954912912471522"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(PR_,y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on document Plan_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('task2_train_dict','rb')\n",
    "task2_test_dict = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ALLREPORTS2/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(filename, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_index_list = list(task2_test_dict[filename.split('/')[-1]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "last = 0\n",
    "for var in abbr_index_list:\n",
    "    begin = int(var[0])\n",
    "    end = int(var[1])\n",
    "    output = output + document[last:begin] + \"@@@\"\n",
    "    last = end\n",
    "output = output + document[last:len(document)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on document Plan_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/output_2/test_bio/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'O': 0, 'Disorder': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(filename)\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in lines:\n",
    "    text.append(line.split('\\t')[0])\n",
    "    label.append(line.split('\\t')[-1].replace('\\n',''))        \n",
    "new_label = []\n",
    "for var in label:       \n",
    "    new_label.append(label_dict[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_groups(init_list, children_list_len):\n",
    "    list_of_groups = zip(*(iter(init_list),) *children_list_len)\n",
    "    end_list = [list(i) for i in list_of_groups]\n",
    "    count = len(init_list) % children_list_len\n",
    "    end_list.append(init_list[-count:]) if count !=0 else end_list\n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_4 = list_of_groups(text,4)\n",
    "new_text_by_4 = [' '.join(i) for i in text_by_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 4)\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "X = t.texts_to_sequences(new_text_by_4)\n",
    "X = pad_sequences(X, maxlen=4)\n",
    "\n",
    "#print(X)\n",
    "PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "#print(PREDICTION.shape) #(4, m, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "#print(PREDICTION.shape) #(m, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "print(len(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneline_prediction = PR.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneline_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750000000000001"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(new_label,oneline_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
