{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob, os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import os\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, GRU, Concatenate, Permute, Dot, Multiply\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from nmt_utils import *\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/ennoh/Desktop/Detector/data/'\n",
    "\n",
    "raw_training = working_dir+'ALLREPORTS/'\n",
    "labels_training = working_dir+'ShAReCLEFeHealthAA_1stRoundRS_CLEFPIPEDELIMITED_200_CUIs/'\n",
    "raw_testing =  working_dir+'ALLREPORTS2/'\n",
    "#labels_testing =working_dir+'Task2ReferenceStd_CLEFShARe2013Test_StrictAndLenientpipe/'\n",
    "labels_testing =working_dir+ 'Task2ReferenceStd_CLEFShARe2013Test_TestSpansOnlypipe/'\n",
    "\n",
    "\n",
    "output = working_dir+'output_2/'\n",
    "train_bio = output+\"train_bio/\"\n",
    "test_bio = output+\"test_bio/\"\n",
    "train_tags_dict = output+'train_tags_dict'\n",
    "test_tags_dict = output+'test_tags_dict'\n",
    "\n",
    "train_doc_file = output+'train_docs.pkl'\n",
    "train_lable_file = output+'train_labels.pkl'\n",
    "train_lable_index_file = output+'train_label_index.pkl'\n",
    "\n",
    "test_doc_file = output+'test_docs.pkl'\n",
    "test_lable_file = output+'test_labels.pkl'\n",
    "test_lable_index_file = output+'test_label_index.pkl'\n",
    "\n",
    "label_dict_file = output+'label_dict.pickle'\n",
    "\n",
    "SLIDING_WINDOW = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(l, size=7):\n",
    "    \"\"\"\n",
    "    Wraps up the input string.\n",
    "    \"\"\"\n",
    "    l = list(l)\n",
    "    #左右一起补(size/2)个0\n",
    "    lpadded = size // 2 * [0] + l + size // 2 * [0]\n",
    "    \n",
    "    out = [lpadded[i:(i + size)] for i in range(len(l))]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in both train and test files\n",
    "\n",
    "#Training\n",
    "fo = open(train_doc_file,'rb')\n",
    "training_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_index_file,'rb')\n",
    "training_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_file,'rb')\n",
    "training_label_instances = pickle.load(fo)\n",
    "\n",
    "#Testing\n",
    "fo = open(test_doc_file,'rb')\n",
    "test_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_index_file,'rb')\n",
    "test_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_file,'rb')\n",
    "test_label_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(label_dict_file,'rb')\n",
    "label_dict = pickle.load(fo)\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training examples into a single file\n",
    "X_trn=[]\n",
    "for s in training_file_instances:\n",
    "    X_trn += context(s,size=SLIDING_WINDOW)\n",
    "X_trn = np.array(X_trn)\n",
    "\n",
    "y_trn=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn += list(s)\n",
    "y_trn = np.array(y_trn)\n",
    "\n",
    "#Convert testing examples into a single file\n",
    "X_tst=[]\n",
    "for s in test_file_instances:\n",
    "    X_tst += context(s,size=SLIDING_WINDOW)\n",
    "X_tst = np.array(X_tst)\n",
    "\n",
    "y_tst=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst += list(s)\n",
    "y_tst = np.array(y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_trn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_seq=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_trn_seq = np.array(y_trn_seq)\n",
    "\n",
    "y_tst_seq=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_tst_seq = np.array(y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '17830', '||||'],\n",
       "       ['0', '17830', '||||', '23590'],\n",
       "       ['17830', '||||', '23590', '||||'],\n",
       "       ...,\n",
       "       ['end', 'of', 'report', '¤'],\n",
       "       ['of', 'report', '¤', '¤'],\n",
       "       ['report', '¤', '¤', '0']], dtype='<U30')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 4\n",
    "train_texts = [' '.join(text) for text in X_trn]\n",
    "test_texts = [' '.join(text) for text in X_tst]\n",
    "\n",
    "all_texts = train_texts + test_texts\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(all_texts)\n",
    "word_to_index = t.word_index\n",
    "vocab_len = len(word_to_index) + 1\n",
    "encoded_X_train = t.texts_to_sequences(train_texts)\n",
    "encoded_X_test = t.texts_to_sequences(test_texts)\n",
    "\n",
    "#padding\n",
    "encoded_X_train = pad_sequences(encoded_X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "encoded_X_test = pad_sequences(encoded_X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,    20,    20, 10765],\n",
       "       [    0,    20, 10765,  6142],\n",
       "       [    0,     0, 10765,  6142],\n",
       "       ...,\n",
       "       [  199,     6,    69,     1],\n",
       "       [    6,    69,     1,     1],\n",
       "       [   69,     1,     1,    20]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape: (115916, 4)\n",
      "y_trn.shape: (115916,)\n",
      "y_trn_seq.shape: (115916, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trn.shape:\", X_trn.shape)\n",
    "print(\"y_trn.shape:\", y_trn.shape)\n",
    "print(\"y_trn_seq.shape:\", y_trn_seq.shape)\n",
    "\n",
    "#print(\"Xoh.shape:\", Xoh.shape)\n",
    "#print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: ['04' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 96 251 360   1]\n",
      "\n",
      "Target date: 1\n",
      "Target date (seq): [0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source date:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target date:\", y_trn[index])\n",
    "print(\"Target date (seq):\", y_trn_seq[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "#print(\"Target after preprocessing (indices):\", encoded_X_test[index])\n",
    "#print()\n",
    "#print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "#print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1,2,3]\n",
    "convert_to_one_hot(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [0,0,1,0]\n",
    "convert_to_one_hot(b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoh_train = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_trn_seq)))\n",
    "Yoh_test = np.array(list(map(lambda x: to_categorical(x, num_classes=2), y_tst_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115916, 4, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108856, 4, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['04' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 96 251 360   1]\n",
      "\n",
      "Target data: 1\n",
      "Target data (seq): [0 0 1 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source data:\", X_trn[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_train[index])\n",
    "print()\n",
    "print(\"Target data:\", y_trn[index])\n",
    "print(\"Target data (seq):\", y_trn_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['20' 'sex' 'f' '¤']\n",
      "Source after preprocessing (indices): [ 98 251 360   1]\n",
      "\n",
      "Target data: 1\n",
      "Target data (seq): [0 0 1 0]\n",
      "Target data (one-hot): [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 40\n",
    "print(\"Source data:\", X_tst[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_test[index])\n",
    "print()\n",
    "print(\"Target data:\", y_tst[index])\n",
    "print(\"Target data (seq):\", y_tst_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_test[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model = FastText.load_fasttext_format('mimic_3_fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    #emb_dim = 200\n",
    "    #len(word_to_index) = 11053\n",
    "    #vocab_len = 11054\n",
    "    \n",
    "    emb_dim = mimic_model[\"ca\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in mimic_model.wv.vocab:\n",
    "            emb_matrix[index, :] = mimic_model[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "            \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21576422"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_weights()[0][3][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "Tx = 4\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "#output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "output_layer = Dense(2, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"      embedding_dim\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"      label_number\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    \n",
    "    sentence_indices = Input(shape=(Tx,), dtype=np.int32)\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    #X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "        \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([sentence_indices, s0, c0], outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(4, 4, n_a, n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4, 200)       2210800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 4, 128)       135680      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 4, 128)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 256)       0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4, 1)         257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 4, 1)         0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,478,579\n",
      "Trainable params: 2,478,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "115916/115916 [==============================] - 90s 774us/step - loss: 0.1499 - dense_2_loss: 0.0352 - dense_2_acc: 0.9868 - dense_2_acc_1: 0.9840 - dense_2_acc_2: 0.9864 - dense_2_acc_3: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b141ded30>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoded_X_train, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  27  100    3 2013]\n",
      " [  27   14 2113 3346]\n",
      " [  96  251  360    1]\n",
      " [ 100    3 2013  118]\n",
      " [   3   84   14   83]\n",
      " [  84   14   83  282]]\n",
      "(4, 6, 2)\n",
      "(6, 4, 2)\n",
      "(6, 4)\n",
      "Prediction: \n",
      "[[0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['she has the ca', 'she is working hard', '04 sex f ¤', 'has the ca but', 'the pt is not', 'pt is not good']\n",
    "\n",
    "EXAMPLES = t.texts_to_sequences(EXAMPLES)\n",
    "EXAMPLES = pad_sequences(EXAMPLES, maxlen=4)\n",
    "\n",
    "print(EXAMPLES)\n",
    "PREDICTION = model.predict([EXAMPLES, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "print(PREDICTION.shape) #(4, 3, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "print(PREDICTION.shape) #(3, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "\n",
    "#print('User Input: ' , E)\n",
    "print('Prediction: ')\n",
    "print(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 108856, 2)\n",
      "(108856, 4, 2)\n",
      "(108856, 4)\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_ = model.predict([encoded_X_test, s0, c0])\n",
    "\n",
    "#print(PREDICTION_)\n",
    "PREDICTION_ = np.asarray(PREDICTION_)\n",
    "print(PREDICTION_.shape) #(4, 108856, 2)\n",
    "PREDICTION_ = PREDICTION_.swapaxes(0,1)\n",
    "print(PREDICTION_.shape) #(108856, 4, 2)\n",
    "\n",
    "PR_ = np.argmax(PREDICTION_, axis = -1)\n",
    "print(PR_.shape)\n",
    "\n",
    "print(PR_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([[0,0,0,0],[0,0,0,1]])\n",
    "b = np.asarray([[0,0,0,0],[0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=a,y_pred=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555743367384435"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(PR_,y_tst_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(PR_,y_tst_seq,average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('task2_train_dict','rb')\n",
    "task2_test_dict = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ALLREPORTS2/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(filename, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'102920\\t||||\\t176\\t||||\\t2167\\t||||\\tECHO_REPORT\\t||||\\t2013-09-23 00:00:00.0\\t||||\\t\\t||||\\t\\nPATIENT/TEST INFORMATION:\\nIndication: Pericardial effusion. \\nHeight: (in) 68\\nWeight (lb): 184\\nBSA (m2): 1.97 m2\\nBP (mm Hg): 140/80\\nHR (bpm): 70\\nStatus: Inpatient\\nDate/Time: [**2013-09-23**] at 10:20\\nTest: TTE (Focused views)\\nDoppler: No doppler\\nContrast: None\\nTechnical Quality: Adequate\\n\\n||||\\t\\t||||\\t\\nINTERPRETATION: \\n\\nFindings: \\n\\nLEFT ATRIUM: The left atrium is moderately dilated. The left atrium is elongated. \\n\\nRIGHT ATRIUM/INTERATRIAL SEPTUM: The right atrium is moderately dilated. \\n\\nLEFT VENTRICLE: There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (LVEF 50-55%). \\n\\nLV WALL MOTION: The following resting regional left ventricular wall motion abnormalities are seen: basal inferoseptal - hypokinetic; mid inferoseptal - hypokinetic; basal inferior - hypokinetic; mid inferior - hypokinetic; \\n\\nRIGHT VENTRICLE: The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. \\n\\nAORTIC VALVE: The aortic valve leaflets (3) are mildly thickened. \\n\\nMITRAL VALVE: The mitral valve leaflets are mildly thickened. There is mild mitral annular calcification. \\n\\nTRICUSPID VALVE: Mild tricuspid [1+] regurgitation is seen. There is mild pulmonary artery systolic hypertension. \\n\\nPERICARDIUM: There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n\\nConclusions: \\n1. Limited focused study. \\n2. The left atrium is moderately dilated. The left atrium is elongated. \\n3.The right atrium is moderately dilated. \\n4.There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (LVEF 50-55%). Resting regional wall motion abnormalities include basal and mid septal hypokinesis. \\n5.The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. 6.The aortic valve leaflets (3) are mildly thickened. No color doppler study was done to assess for the presence of MR [**First Name (Titles) 4**] [**Last Name (Titles) 15**]. 7. The mitral valve leaflets are mildly thickened. \\n8.There is mild pulmonary artery systolic hypertension. There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n9. An echogenic density in the right ventricle is present consistent with a pacemaker lead. \\n\\nCompared with the findings of the prior study (tape reviewed) of [**2013-09-12**], the pericardial effusion is much less. The overall function is unchanged. \\n\\n'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_index_list = list(task2_test_dict[filename.split('/')[-1]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 35),\n",
       " (176, 179),\n",
       " (194, 196),\n",
       " (213, 215),\n",
       " (287, 290),\n",
       " (746, 750),\n",
       " (762, 764),\n",
       " (1861, 1865),\n",
       " (2218, 2220)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbr_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "last = 0\n",
    "for var in abbr_index_list:\n",
    "    begin = int(var[0])\n",
    "    end = int(var[1])\n",
    "    output = output + document[last:begin] + \"@@@\"\n",
    "    last = end\n",
    "output = output + document[last:len(document)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'102920\\t||||\\t176\\t||||\\t2167\\t||||\\t@@@_REPORT\\t||||\\t2013-09-23 00:00:00.0\\t||||\\t\\t||||\\t\\nPATIENT/TEST INFORMATION:\\nIndication: Pericardial effusion. \\nHeight: (in) 68\\nWeight (lb): 184\\n@@@ (m2): 1.97 m2\\n@@@ (mm Hg): 140/80\\n@@@ (bpm): 70\\nStatus: Inpatient\\nDate/Time: [**2013-09-23**] at 10:20\\nTest: @@@ (Focused views)\\nDoppler: No doppler\\nContrast: None\\nTechnical Quality: Adequate\\n\\n||||\\t\\t||||\\t\\nINTERPRETATION: \\n\\nFindings: \\n\\nLEFT ATRIUM: The left atrium is moderately dilated. The left atrium is elongated. \\n\\nRIGHT ATRIUM/INTERATRIAL SEPTUM: The right atrium is moderately dilated. \\n\\nLEFT VENTRICLE: There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (@@@ 50-55%). \\n\\n@@@ WALL MOTION: The following resting regional left ventricular wall motion abnormalities are seen: basal inferoseptal - hypokinetic; mid inferoseptal - hypokinetic; basal inferior - hypokinetic; mid inferior - hypokinetic; \\n\\nRIGHT VENTRICLE: The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. \\n\\nAORTIC VALVE: The aortic valve leaflets (3) are mildly thickened. \\n\\nMITRAL VALVE: The mitral valve leaflets are mildly thickened. There is mild mitral annular calcification. \\n\\nTRICUSPID VALVE: Mild tricuspid [1+] regurgitation is seen. There is mild pulmonary artery systolic hypertension. \\n\\nPERICARDIUM: There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n\\nConclusions: \\n1. Limited focused study. \\n2. The left atrium is moderately dilated. The left atrium is elongated. \\n3.The right atrium is moderately dilated. \\n4.There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (@@@ 50-55%). Resting regional wall motion abnormalities include basal and mid septal hypokinesis. \\n5.The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. 6.The aortic valve leaflets (3) are mildly thickened. No color doppler study was done to assess for the presence of @@@ [**First Name (Titles) 4**] [**Last Name (Titles) 15**]. 7. The mitral valve leaflets are mildly thickened. \\n8.There is mild pulmonary artery systolic hypertension. There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n9. An echogenic density in the right ventricle is present consistent with a pacemaker lead. \\n\\nCompared with the findings of the prior study (tape reviewed) of [**2013-09-12**], the pericardial effusion is much less. The overall function is unchanged. \\n\\n'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'102920\\t||||\\t176\\t||||\\t2167\\t||||\\tECHO_REPORT\\t||||\\t2013-09-23 00:00:00.0\\t||||\\t\\t||||\\t\\nPATIENT/TEST INFORMATION:\\nIndication: Pericardial effusion. \\nHeight: (in) 68\\nWeight (lb): 184\\nBSA (m2): 1.97 m2\\nBP (mm Hg): 140/80\\nHR (bpm): 70\\nStatus: Inpatient\\nDate/Time: [**2013-09-23**] at 10:20\\nTest: TTE (Focused views)\\nDoppler: No doppler\\nContrast: None\\nTechnical Quality: Adequate\\n\\n||||\\t\\t||||\\t\\nINTERPRETATION: \\n\\nFindings: \\n\\nLEFT ATRIUM: The left atrium is moderately dilated. The left atrium is elongated. \\n\\nRIGHT ATRIUM/INTERATRIAL SEPTUM: The right atrium is moderately dilated. \\n\\nLEFT VENTRICLE: There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (LVEF 50-55%). \\n\\nLV WALL MOTION: The following resting regional left ventricular wall motion abnormalities are seen: basal inferoseptal - hypokinetic; mid inferoseptal - hypokinetic; basal inferior - hypokinetic; mid inferior - hypokinetic; \\n\\nRIGHT VENTRICLE: The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. \\n\\nAORTIC VALVE: The aortic valve leaflets (3) are mildly thickened. \\n\\nMITRAL VALVE: The mitral valve leaflets are mildly thickened. There is mild mitral annular calcification. \\n\\nTRICUSPID VALVE: Mild tricuspid [1+] regurgitation is seen. There is mild pulmonary artery systolic hypertension. \\n\\nPERICARDIUM: There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n\\nConclusions: \\n1. Limited focused study. \\n2. The left atrium is moderately dilated. The left atrium is elongated. \\n3.The right atrium is moderately dilated. \\n4.There is severe symmetric left ventricular hypertrophy. The left ventricular cavity size is normal. Overall left ventricular systolic function is low normal (LVEF 50-55%). Resting regional wall motion abnormalities include basal and mid septal hypokinesis. \\n5.The right ventricular free wall is hypertrophied. Right ventricular chamber size is normal. Right ventricular systolic function is normal. 6.The aortic valve leaflets (3) are mildly thickened. No color doppler study was done to assess for the presence of MR [**First Name (Titles) 4**] [**Last Name (Titles) 15**]. 7. The mitral valve leaflets are mildly thickened. \\n8.There is mild pulmonary artery systolic hypertension. There is a trivial/physiologic pericardial effusion. The effusion appears circumferential. \\n9. An echogenic density in the right ventricle is present consistent with a pacemaker lead. \\n\\nCompared with the findings of the prior study (tape reviewed) of [**2013-09-12**], the pericardial effusion is much less. The overall function is unchanged. \\n\\n'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
