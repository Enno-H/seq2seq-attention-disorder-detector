{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob, os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import os\n",
    "import datetime\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, GRU, Concatenate, Permute, Dot, Multiply\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "from keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from nmt_utils import *\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/ennoh/Desktop/Detector/data/'\n",
    "\n",
    "raw_training = working_dir+'ALLREPORTS/'\n",
    "labels_training = working_dir+'CLEFPIPEDELIMITED_NoDuplicates/'\n",
    "raw_testing =  working_dir+'ALLREPORTS2/'\n",
    "#labels_testing =working_dir+'Task2ReferenceStd_CLEFShARe2013Test_StrictAndLenientpipe/'\n",
    "labels_testing =working_dir+ 'Gold_SN2011/'\n",
    "\n",
    "\n",
    "output = working_dir+'output_disorder_detector/'\n",
    "train_bio = output+\"train_bio/\"\n",
    "test_bio = output+\"test_bio/\"\n",
    "train_tags_dict = output+'train_tags_dict'\n",
    "test_tags_dict = output+'test_tags_dict'\n",
    "\n",
    "train_doc_file = output+'train_docs.pkl'\n",
    "train_lable_file = output+'train_labels.pkl'\n",
    "train_lable_index_file = output+'train_label_index.pkl'\n",
    "\n",
    "test_doc_file = output+'test_docs.pkl'\n",
    "test_lable_file = output+'test_labels.pkl'\n",
    "test_lable_index_file = output+'test_label_index.pkl'\n",
    "\n",
    "label_dict_file = output+'label_dict.pickle'\n",
    "\n",
    "SLIDING_WINDOW = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(l, size):\n",
    "    \"\"\"\n",
    "    Wraps up the input string.\n",
    "    \"\"\"\n",
    "    l = list(l)\n",
    "    #左右一起补(size/2)个0\n",
    "    lpadded = size // 2 * [0] + l + size // 2 * [0]\n",
    "    \n",
    "    out = [lpadded[i:(i + size)] for i in range(len(l))]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in both train and test files\n",
    "\n",
    "#Training\n",
    "fo = open(train_doc_file,'rb')\n",
    "training_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_index_file,'rb')\n",
    "training_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(train_lable_file,'rb')\n",
    "training_label_instances = pickle.load(fo)\n",
    "\n",
    "#Testing\n",
    "fo = open(test_doc_file,'rb')\n",
    "test_file_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_index_file,'rb')\n",
    "test_label_index_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(test_lable_file,'rb')\n",
    "test_label_instances = pickle.load(fo)\n",
    "\n",
    "fo = open(label_dict_file,'rb')\n",
    "label_dict = pickle.load(fo)\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training examples into a single file\n",
    "X_trn=[]\n",
    "for s in training_file_instances:\n",
    "    X_trn += context(s,size=SLIDING_WINDOW)\n",
    "X_trn = np.array(X_trn)\n",
    "\n",
    "y_trn=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn += list(s)\n",
    "y_trn = np.array(y_trn)\n",
    "\n",
    "#Convert testing examples into a single file\n",
    "X_tst=[]\n",
    "for s in test_file_instances:\n",
    "    X_tst += context(s,size=SLIDING_WINDOW)\n",
    "X_tst = np.array(X_tst)\n",
    "\n",
    "y_tst=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst += list(s)\n",
    "y_tst = np.array(y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_trn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_seq=[]\n",
    "for s in training_label_index_instances:\n",
    "    y_trn_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_trn_seq = np.array(y_trn_seq)\n",
    "\n",
    "y_tst_seq=[]\n",
    "for s in test_label_index_instances:\n",
    "    y_tst_seq += context(s,size=SLIDING_WINDOW)\n",
    "y_tst_seq = np.array(y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', '0', '0', '0', '0', '17830', 'llll', '23590',\n",
       "       'llll', '17919', 'llll', 'discharge', 'summary'], dtype='<U30')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = SLIDING_WINDOW\n",
    "train_texts = [' '.join(text) for text in X_trn]\n",
    "test_texts = [' '.join(text) for text in X_tst]\n",
    "\n",
    "all_texts = train_texts + test_texts\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(all_texts)\n",
    "word_to_index = t.word_index\n",
    "vocab_len = len(word_to_index) + 1\n",
    "encoded_X_train = t.texts_to_sequences(train_texts)\n",
    "encoded_X_test = t.texts_to_sequences(test_texts)\n",
    "\n",
    "#padding\n",
    "encoded_X_train = pad_sequences(encoded_X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "encoded_X_test = pad_sequences(encoded_X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12,  12,  12, ...,  10,  27, 218],\n",
       "       [ 12,  12,  12, ...,  27, 218,  10],\n",
       "       [ 12,  12,  12, ..., 218,  10, 431],\n",
       "       ...,\n",
       "       [127,  23, 918, ...,  12,  12,  12],\n",
       "       [ 23, 918, 246, ...,  12,  12,  12],\n",
       "       [918, 246,   1, ...,  12,  12,  12]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn.shape: (115916, 15)\n",
      "y_trn.shape: (115916,)\n",
      "y_trn_seq.shape: (115916, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trn.shape:\", X_trn.shape)\n",
    "print(\"y_trn.shape:\", y_trn.shape)\n",
    "print(\"y_trn_seq.shape:\", y_trn_seq.shape)\n",
    "\n",
    "#print(\"Xoh.shape:\", Xoh.shape)\n",
    "#print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [0,0,1,0,2]\n",
    "convert_to_one_hot(b, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoh_train = np.array(list(map(lambda x: to_categorical(x, num_classes=len(label_dict)), y_trn_seq)))\n",
    "Yoh_test = np.array(list(map(lambda x: to_categorical(x, num_classes=len(label_dict)), y_tst_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115916, 15, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108856, 15, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data: ['also' 'denied' 'paroxysmal' 'nocturnal' 'dyspnea' 'or' 'orthopnea' 'she'\n",
      " 'denied' 'shortness' 'of' 'breath' 'but' 'did' 'report']\n",
      "Source after preprocessing (indices): [ 128 1046 2246 3259 1161   46 2677   28 1046  464    6  329  119  300\n",
      "   78]\n",
      "\n",
      "Target data: 0\n",
      "Target data (seq): [0 0 1 2 2 0 1 0 0 1 2 2 0 0 0]\n",
      "Target data (one-hot): [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "print(\"Source data:\", X_tst[index])\n",
    "print(\"Source after preprocessing (indices):\", encoded_X_test[index])\n",
    "print()\n",
    "print(\"Target data:\", y_tst[index])\n",
    "print(\"Target data (seq):\", y_tst_seq[index])\n",
    "print(\"Target data (one-hot):\", Yoh_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9187"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_model = FastText.load_fasttext_format('mimic_3_fasttext.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer():\n",
    "    \n",
    "    #emb_dim = 200\n",
    "    #len(word_to_index) = 11053\n",
    "    #vocab_len = 11054\n",
    "    \n",
    "    emb_dim = mimic_model[\"ca\"].shape[0] \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in mimic_model.wv.vocab:\n",
    "            emb_matrix[index, :] = mimic_model[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "            \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pretrained_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21576422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_weights()[0][3][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "Tx = SLIDING_WINDOW\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "#output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "output_layer = Dense(len(label_dict), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"      embedding_dim\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"      label_number\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    \n",
    "    sentence_indices = Input(shape=(Tx,), dtype=np.int32)\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer()\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices) \n",
    "    \n",
    "    #X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "        \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([sentence_indices, s0, c0], outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(SLIDING_WINDOW, SLIDING_WINDOW, n_a, n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 15, 200)      2211000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 15, 128)      135680      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 15, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[14][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 15, 1)        257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "                                                                 concatenate_1[10][0]             \n",
      "                                                                 concatenate_1[11][0]             \n",
      "                                                                 concatenate_1[12][0]             \n",
      "                                                                 concatenate_1[13][0]             \n",
      "                                                                 concatenate_1[14][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 15, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 dot_1[10][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 dot_1[14][0]                     \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,478,908\n",
      "Trainable params: 2,478,908\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "115916/115916 [==============================] - 231s 2ms/step - loss: 0.1455 - dense_2_loss: 0.0351 - dense_2_acc: 0.9878 - dense_2_acc_1: 0.9849 - dense_2_acc_2: 0.9860 - dense_2_acc_3: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a500cca90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoded_X_train, s0, c0], outputs, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 's0:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'c0:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_1/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/TensorArrayReadV3:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_4:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "#model.save('model/model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/model_Task1_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0   28  101    3\n",
      "  2014]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   28   16 2110\n",
      "  3347]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   96  251  361\n",
      "     1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  101    3 2014\n",
      "   119]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    3   85   16\n",
      "    84]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   85   16   84\n",
      "   282]]\n",
      "(15, 6, 3)\n",
      "(6, 15, 3)\n",
      "(6, 15)\n",
      "Prediction: \n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 1 2 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['she has the ca', 'she is working hard', '04 sex f ¤', 'has the ca but', 'the pt is not', 'pt is not good']\n",
    "\n",
    "EXAMPLES = t.texts_to_sequences(EXAMPLES)\n",
    "EXAMPLES = pad_sequences(EXAMPLES, maxlen = SLIDING_WINDOW)\n",
    "\n",
    "print(EXAMPLES)\n",
    "PREDICTION = model.predict([EXAMPLES, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "print(PREDICTION.shape) #(4, 3, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "print(PREDICTION.shape) #(3, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "\n",
    "#print('User Input: ' , E)\n",
    "print('Prediction: ')\n",
    "print(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 108856, 3)\n",
      "(108856, 15, 3)\n",
      "(108856, 15)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "PREDICTION_ = model.predict([encoded_X_test, s0, c0])\n",
    "\n",
    "#print(PREDICTION_)\n",
    "PREDICTION_ = np.asarray(PREDICTION_)\n",
    "print(PREDICTION_.shape) #(4, 108856, 2)\n",
    "PREDICTION_ = PREDICTION_.swapaxes(0,1)\n",
    "print(PREDICTION_.shape) #(108856, 4, 2)\n",
    "\n",
    "PR_ = np.argmax(PREDICTION_, axis = -1)\n",
    "print(PR_.shape)\n",
    "\n",
    "print(PR_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   21,    21, 10963,     8],\n",
       "       [   21, 10963,     8,  8528],\n",
       "       [10963,     8,  8528,     8],\n",
       "       ...,\n",
       "       [  200,     6,    70,     1],\n",
       "       [    6,    70,     1,     1],\n",
       "       [   70,     1,     1,    21]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([[0,0,0,0],[0,0,0,1]])\n",
    "b = np.asarray([[0,0,0,0],[0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=a,y_pred=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955206878812376"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(PR_,y_tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on document Plan_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('task2_train_dict','rb')\n",
    "task2_test_dict = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ALLREPORTS2/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(filename, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_index_list = list(task2_test_dict[filename.split('/')[-1]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "last = 0\n",
    "for var in abbr_index_list:\n",
    "    begin = int(var[0])\n",
    "    end = int(var[1])\n",
    "    output = output + document[last:begin] + \"@@@\"\n",
    "    last = end\n",
    "output = output + document[last:len(document)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on '00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/output_disorder_detector/test_bio/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(filename)\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in lines:\n",
    "    text.append(line.split('\\t')[0])\n",
    "    label.append(line.split('\\t')[-1].replace('\\n',''))        \n",
    "new_label = []\n",
    "for var in label:       \n",
    "    new_label.append(label_dict[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n",
      "452\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(label))\n",
    "print(len(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['102920','llll','176','llll','2167','llll','echo','report','llll',..]  ==> [['102920', 'llll', '176', 'llll'],['2167', 'llll', 'echo', 'report'],...]\n",
    "def list_of_groups(init_list, children_list_len):\n",
    "    list_of_groups = zip(*(iter(init_list),) *children_list_len)\n",
    "    end_list = [list(i) for i in list_of_groups]\n",
    "    count = len(init_list) % children_list_len\n",
    "    end_list.append(init_list[-count:]) if count !=0 else end_list\n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_slice = list_of_groups(text, MAX_SEQUENCE_LENGTH)\n",
    "new_text_slice = [' '.join(i) for i in text_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 15)\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "X = t.texts_to_sequences(new_text_slice)\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#print(X)\n",
    "PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "#print(PREDICTION)\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "#print(PREDICTION.shape) #(4, m, 2)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "#print(PREDICTION.shape) #(m, 4, 2)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "print(PR.shape)\n",
    "print(len(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneline_prediction = PR.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneline_prediction = oneline_prediction[:len(new_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oneline_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9756637168141593"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(new_label,oneline_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9756637168141593"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(new_label, oneline_prediction, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741981111656308"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(new_label, oneline_prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9756637168141593"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(new_label, oneline_prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on 99 reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_single_file(filename):\n",
    "    text = []\n",
    "    label = []\n",
    "    new_label = []\n",
    "    f = open(filename)\n",
    "    for line in f.readlines():\n",
    "        text.append(line.split('\\t')[0])\n",
    "        label.append(line.split('\\t')[-1].replace('\\n',''))        \n",
    "    for var in label:       \n",
    "        new_label.append(label_dict[var])\n",
    "    text_by_4 = list_of_groups(text,MAX_SEQUENCE_LENGTH)\n",
    "    new_text_by_4 = [' '.join(i) for i in text_by_4]\n",
    "    \n",
    "    X = t.texts_to_sequences(new_text_by_4)\n",
    "    X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "    PREDICTION = np.asarray(PREDICTION)\n",
    "    PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "\n",
    "    PR = np.argmax(PREDICTION, axis = -1)\n",
    "    oneline_prediction = PR.flatten()\n",
    "    oneline_prediction = oneline_prediction[:len(new_label)]\n",
    "    \n",
    "    print(filename.split('/')[-1],'  Number: ',len(new_label),'  F1:',f1_score(new_label,oneline_prediction, average='micro'), '  Presicion: ',precision_score(new_label,oneline_prediction, average='micro'), '  Recall: ',recall_score(new_label,oneline_prediction, average='micro'))\n",
    "    return f1_score(new_label,oneline_prediction, average='micro'), new_label, list(oneline_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02740-024700-DISCHARGE_SUMMARY.txt   Number:  902   F1: 0.9634146341463414   Presicion:  0.9634146341463414   Recall:  0.9634146341463414\n",
      "09703-109051-ECHO_REPORT.txt   Number:  453   F1: 0.9757174392935982   Presicion:  0.9757174392935982   Recall:  0.9757174392935982\n",
      "03087-026480-DISCHARGE_SUMMARY.txt   Number:  1346   F1: 0.987369985141159   Presicion:  0.987369985141159   Recall:  0.987369985141159\n",
      "08990-002227-DISCHARGE_SUMMARY.txt   Number:  2283   F1: 0.9583880858519492   Presicion:  0.9583880858519492   Recall:  0.9583880858519492\n",
      "19138-025729-DISCHARGE_SUMMARY.txt   Number:  1232   F1: 0.9285714285714286   Presicion:  0.9285714285714286   Recall:  0.9285714285714286\n",
      "04082-167766-RADIOLOGY_REPORT.txt   Number:  302   F1: 0.9867549668874173   Presicion:  0.9867549668874173   Recall:  0.9867549668874173\n",
      "17652-018982-DISCHARGE_SUMMARY.txt   Number:  1582   F1: 0.9690265486725663   Presicion:  0.9690265486725663   Recall:  0.9690265486725663\n",
      "21951-203738-RADIOLOGY_REPORT.txt   Number:  267   F1: 0.902621722846442   Presicion:  0.9026217228464419   Recall:  0.9026217228464419\n",
      "16997-000825-DISCHARGE_SUMMARY.txt   Number:  1597   F1: 0.9611772072636193   Presicion:  0.9611772072636193   Recall:  0.9611772072636193\n",
      "00176-102920-ECHO_REPORT.txt   Number:  452   F1: 0.9756637168141593   Presicion:  0.9756637168141593   Recall:  0.9756637168141593\n",
      "21312-018707-DISCHARGE_SUMMARY.txt   Number:  1337   F1: 0.9648466716529543   Presicion:  0.9648466716529543   Recall:  0.9648466716529543\n",
      "03298-014440-DISCHARGE_SUMMARY.txt   Number:  876   F1: 0.95662100456621   Presicion:  0.95662100456621   Recall:  0.95662100456621\n",
      "00534-017453-DISCHARGE_SUMMARY.txt   Number:  1438   F1: 0.9603616133518776   Presicion:  0.9603616133518776   Recall:  0.9603616133518776\n",
      "05382-010331-DISCHARGE_SUMMARY.txt   Number:  1832   F1: 0.9596069868995634   Presicion:  0.9596069868995634   Recall:  0.9596069868995634\n",
      "09339-028983-DISCHARGE_SUMMARY.txt   Number:  1192   F1: 0.9555369127516778   Presicion:  0.9555369127516778   Recall:  0.9555369127516778\n",
      "05065-011493-DISCHARGE_SUMMARY.txt   Number:  2394   F1: 0.9302422723475355   Presicion:  0.9302422723475355   Recall:  0.9302422723475355\n",
      "13990-101915-ECHO_REPORT.txt   Number:  379   F1: 0.899736147757256   Presicion:  0.899736147757256   Recall:  0.899736147757256\n",
      "11392-010791-DISCHARGE_SUMMARY.txt   Number:  1592   F1: 0.9528894472361809   Presicion:  0.9528894472361809   Recall:  0.9528894472361809\n",
      "18114-360237-RADIOLOGY_REPORT.txt   Number:  296   F1: 0.9087837837837838   Presicion:  0.9087837837837838   Recall:  0.9087837837837838\n",
      "20389-024150-DISCHARGE_SUMMARY.txt   Number:  1737   F1: 0.9568221070811744   Presicion:  0.9568221070811744   Recall:  0.9568221070811744\n",
      "07214-025053-DISCHARGE_SUMMARY.txt   Number:  1502   F1: 0.9553928095872171   Presicion:  0.9553928095872171   Recall:  0.9553928095872171\n",
      "14285-022846-DISCHARGE_SUMMARY.txt   Number:  1137   F1: 0.9138082673702727   Presicion:  0.9138082673702727   Recall:  0.9138082673702727\n",
      "09602-000963-DISCHARGE_SUMMARY.txt   Number:  945   F1: 0.9576719576719578   Presicion:  0.9576719576719577   Recall:  0.9576719576719577\n",
      "00534-100076-ECHO_REPORT.txt   Number:  344   F1: 0.997093023255814   Presicion:  0.997093023255814   Recall:  0.997093023255814\n",
      "16660-004075-DISCHARGE_SUMMARY.txt   Number:  1431   F1: 0.9573724668064291   Presicion:  0.9573724668064291   Recall:  0.9573724668064291\n",
      "03835-028462-DISCHARGE_SUMMARY.txt   Number:  511   F1: 0.9902152641878669   Presicion:  0.9902152641878669   Recall:  0.9902152641878669\n",
      "05163-019624-DISCHARGE_SUMMARY.txt   Number:  879   F1: 0.9749715585893061   Presicion:  0.9749715585893061   Recall:  0.9749715585893061\n",
      "19154-166217-RADIOLOGY_REPORT.txt   Number:  210   F1: 0.9523809523809523   Presicion:  0.9523809523809523   Recall:  0.9523809523809523\n",
      "25775-007416-DISCHARGE_SUMMARY.txt   Number:  2568   F1: 0.942367601246106   Presicion:  0.942367601246106   Recall:  0.942367601246106\n",
      "12530-004020-DISCHARGE_SUMMARY.txt   Number:  1508   F1: 0.9423076923076923   Presicion:  0.9423076923076923   Recall:  0.9423076923076923\n",
      "11552-026221-DISCHARGE_SUMMARY.txt   Number:  1098   F1: 0.9426229508196722   Presicion:  0.9426229508196722   Recall:  0.9426229508196722\n",
      "12125-022364-DISCHARGE_SUMMARY.txt   Number:  1796   F1: 0.9548997772828508   Presicion:  0.9548997772828508   Recall:  0.9548997772828508\n",
      "05837-000274-DISCHARGE_SUMMARY.txt   Number:  1416   F1: 0.9491525423728814   Presicion:  0.9491525423728814   Recall:  0.9491525423728814\n",
      "08415-016301-DISCHARGE_SUMMARY.txt   Number:  1530   F1: 0.9555555555555556   Presicion:  0.9555555555555556   Recall:  0.9555555555555556\n",
      "11681-022505-DISCHARGE_SUMMARY.txt   Number:  1610   F1: 0.924223602484472   Presicion:  0.924223602484472   Recall:  0.924223602484472\n",
      "16677-010128-DISCHARGE_SUMMARY.txt   Number:  370   F1: 0.9621621621621622   Presicion:  0.9621621621621622   Recall:  0.9621621621621622\n",
      "17774-014129-DISCHARGE_SUMMARY.txt   Number:  1235   F1: 0.977327935222672   Presicion:  0.977327935222672   Recall:  0.977327935222672\n",
      "07683-016743-DISCHARGE_SUMMARY.txt   Number:  1199   F1: 0.9291075896580484   Presicion:  0.9291075896580484   Recall:  0.9291075896580484\n",
      "22788-021533-DISCHARGE_SUMMARY.txt   Number:  1471   F1: 0.964649898028552   Presicion:  0.964649898028552   Recall:  0.964649898028552\n",
      "16743-013010-DISCHARGE_SUMMARY.txt   Number:  364   F1: 0.9807692307692307   Presicion:  0.9807692307692307   Recall:  0.9807692307692307\n",
      "15230-012950-DISCHARGE_SUMMARY.txt   Number:  1136   F1: 0.9427816901408451   Presicion:  0.9427816901408451   Recall:  0.9427816901408451\n",
      "15664-014779-DISCHARGE_SUMMARY.txt   Number:  532   F1: 0.9868421052631579   Presicion:  0.9868421052631579   Recall:  0.9868421052631579\n",
      "18531-010240-DISCHARGE_SUMMARY.txt   Number:  1680   F1: 0.9232142857142858   Presicion:  0.9232142857142858   Recall:  0.9232142857142858\n",
      "24307-009748-DISCHARGE_SUMMARY.txt   Number:  1072   F1: 0.9533582089552238   Presicion:  0.9533582089552238   Recall:  0.9533582089552238\n",
      "12627-109059-ECHO_REPORT.txt   Number:  315   F1: 0.9777777777777777   Presicion:  0.9777777777777777   Recall:  0.9777777777777777\n",
      "06557-009968-DISCHARGE_SUMMARY.txt   Number:  943   F1: 0.9512195121951219   Presicion:  0.9512195121951219   Recall:  0.9512195121951219\n",
      "16072-170823-RADIOLOGY_REPORT.txt   Number:  214   F1: 0.9252336448598131   Presicion:  0.9252336448598131   Recall:  0.9252336448598131\n",
      "20223-103427-ECHO_REPORT.txt   Number:  332   F1: 0.9457831325301205   Presicion:  0.9457831325301205   Recall:  0.9457831325301205\n",
      "18108-381702-RADIOLOGY_REPORT.txt   Number:  234   F1: 0.8803418803418802   Presicion:  0.8803418803418803   Recall:  0.8803418803418803\n",
      "16134-204168-RADIOLOGY_REPORT.txt   Number:  390   F1: 0.9102564102564102   Presicion:  0.9102564102564102   Recall:  0.9102564102564102\n",
      "20442-023289-DISCHARGE_SUMMARY.txt   Number:  1173   F1: 0.9744245524296675   Presicion:  0.9744245524296675   Recall:  0.9744245524296675\n",
      "09248-026497-DISCHARGE_SUMMARY.txt   Number:  1736   F1: 0.9464285714285714   Presicion:  0.9464285714285714   Recall:  0.9464285714285714\n",
      "11378-103592-ECHO_REPORT.txt   Number:  381   F1: 0.9396325459317585   Presicion:  0.9396325459317585   Recall:  0.9396325459317585\n",
      "01222-104065-ECHO_REPORT.txt   Number:  307   F1: 0.9609120521172638   Presicion:  0.9609120521172638   Recall:  0.9609120521172638\n",
      "09531-108127-ECHO_REPORT.txt   Number:  342   F1: 0.9853801169590644   Presicion:  0.9853801169590644   Recall:  0.9853801169590644\n",
      "04882-004677-DISCHARGE_SUMMARY.txt   Number:  1786   F1: 0.9641657334826428   Presicion:  0.9641657334826428   Recall:  0.9641657334826428\n",
      "11098-004672-DISCHARGE_SUMMARY.txt   Number:  1165   F1: 0.976824034334764   Presicion:  0.976824034334764   Recall:  0.976824034334764\n",
      "24435-000622-DISCHARGE_SUMMARY.txt   Number:  469   F1: 0.9872068230277186   Presicion:  0.9872068230277186   Recall:  0.9872068230277186\n",
      "17467-010718-DISCHARGE_SUMMARY.txt   Number:  1604   F1: 0.9320448877805486   Presicion:  0.9320448877805486   Recall:  0.9320448877805486\n",
      "06134-005003-DISCHARGE_SUMMARY.txt   Number:  1636   F1: 0.9572127139364304   Presicion:  0.9572127139364304   Recall:  0.9572127139364304\n",
      "15751-026988-DISCHARGE_SUMMARY.txt   Number:  1480   F1: 0.9743243243243244   Presicion:  0.9743243243243244   Recall:  0.9743243243243244\n",
      "10689-110055-ECHO_REPORT.txt   Number:  391   F1: 0.9846547314578005   Presicion:  0.9846547314578005   Recall:  0.9846547314578005\n",
      "17644-017974-DISCHARGE_SUMMARY.txt   Number:  971   F1: 0.9783728115345005   Presicion:  0.9783728115345005   Recall:  0.9783728115345005\n",
      "26522-011368-DISCHARGE_SUMMARY.txt   Number:  1540   F1: 0.9311688311688312   Presicion:  0.9311688311688312   Recall:  0.9311688311688312\n",
      "08216-388895-RADIOLOGY_REPORT.txt   Number:  205   F1: 0.9463414634146341   Presicion:  0.9463414634146341   Recall:  0.9463414634146341\n",
      "22159-004946-DISCHARGE_SUMMARY.txt   Number:  1348   F1: 0.9287833827893175   Presicion:  0.9287833827893175   Recall:  0.9287833827893175\n",
      "19911-175533-RADIOLOGY_REPORT.txt   Number:  196   F1: 0.9336734693877551   Presicion:  0.9336734693877551   Recall:  0.9336734693877551\n",
      "17583-022047-DISCHARGE_SUMMARY.txt   Number:  1309   F1: 0.9343009931245225   Presicion:  0.9343009931245225   Recall:  0.9343009931245225\n",
      "17054-016976-DISCHARGE_SUMMARY.txt   Number:  1410   F1: 0.9411347517730496   Presicion:  0.9411347517730496   Recall:  0.9411347517730496\n",
      "10773-027033-DISCHARGE_SUMMARY.txt   Number:  1443   F1: 0.975051975051975   Presicion:  0.975051975051975   Recall:  0.975051975051975\n",
      "10539-022213-DISCHARGE_SUMMARY.txt   Number:  1698   F1: 0.9605418138987044   Presicion:  0.9605418138987044   Recall:  0.9605418138987044\n",
      "08786-003318-DISCHARGE_SUMMARY.txt   Number:  1905   F1: 0.9417322834645669   Presicion:  0.9417322834645669   Recall:  0.9417322834645669\n",
      "18317-007698-DISCHARGE_SUMMARY.txt   Number:  582   F1: 0.936426116838488   Presicion:  0.936426116838488   Recall:  0.936426116838488\n",
      "01160-000945-DISCHARGE_SUMMARY.txt   Number:  813   F1: 0.9630996309963099   Presicion:  0.9630996309963099   Recall:  0.9630996309963099\n",
      "21979-010316-DISCHARGE_SUMMARY.txt   Number:  1034   F1: 0.9226305609284333   Presicion:  0.9226305609284333   Recall:  0.9226305609284333\n",
      "15789-007213-DISCHARGE_SUMMARY.txt   Number:  954   F1: 0.960167714884696   Presicion:  0.960167714884696   Recall:  0.960167714884696\n",
      "24786-014472-DISCHARGE_SUMMARY.txt   Number:  1079   F1: 0.9675625579240037   Presicion:  0.9675625579240037   Recall:  0.9675625579240037\n",
      "20706-009354-DISCHARGE_SUMMARY.txt   Number:  924   F1: 0.9783549783549783   Presicion:  0.9783549783549783   Recall:  0.9783549783549783\n",
      "04525-003099-DISCHARGE_SUMMARY.txt   Number:  1789   F1: 0.9530463946338736   Presicion:  0.9530463946338736   Recall:  0.9530463946338736\n",
      "03628-023268-DISCHARGE_SUMMARY.txt   Number:  1670   F1: 0.9682634730538923   Presicion:  0.9682634730538923   Recall:  0.9682634730538923\n",
      "21815-002962-DISCHARGE_SUMMARY.txt   Number:  1233   F1: 0.948905109489051   Presicion:  0.948905109489051   Recall:  0.948905109489051\n",
      "07546-000040-DISCHARGE_SUMMARY.txt   Number:  960   F1: 0.9604166666666667   Presicion:  0.9604166666666667   Recall:  0.9604166666666667\n",
      "17097-368450-RADIOLOGY_REPORT.txt   Number:  226   F1: 0.9601769911504425   Presicion:  0.9601769911504425   Recall:  0.9601769911504425\n",
      "04995-028156-DISCHARGE_SUMMARY.txt   Number:  1336   F1: 0.9633233532934131   Presicion:  0.9633233532934131   Recall:  0.9633233532934131\n",
      "08324-097667-ECHO_REPORT.txt   Number:  394   F1: 0.9543147208121827   Presicion:  0.9543147208121827   Recall:  0.9543147208121827\n",
      "16247-028319-DISCHARGE_SUMMARY.txt   Number:  1515   F1: 0.9656765676567657   Presicion:  0.9656765676567657   Recall:  0.9656765676567657\n",
      "20701-013632-DISCHARGE_SUMMARY.txt   Number:  1364   F1: 0.9538123167155426   Presicion:  0.9538123167155426   Recall:  0.9538123167155426\n",
      "12582-011060-DISCHARGE_SUMMARY.txt   Number:  1766   F1: 0.9416761041902605   Presicion:  0.9416761041902605   Recall:  0.9416761041902605\n",
      "19596-007256-DISCHARGE_SUMMARY.txt   Number:  1344   F1: 0.9494047619047619   Presicion:  0.9494047619047619   Recall:  0.9494047619047619\n",
      "01163-001840-DISCHARGE_SUMMARY.txt   Number:  2616   F1: 0.956039755351682   Presicion:  0.956039755351682   Recall:  0.956039755351682\n",
      "25150-027400-DISCHARGE_SUMMARY.txt   Number:  1187   F1: 0.9629317607413648   Presicion:  0.9629317607413648   Recall:  0.9629317607413648\n",
      "16055-152402-RADIOLOGY_REPORT.txt   Number:  265   F1: 0.9358490566037736   Presicion:  0.9358490566037736   Recall:  0.9358490566037736\n",
      "00381-006281-DISCHARGE_SUMMARY.txt   Number:  1351   F1: 0.9489267209474463   Presicion:  0.9489267209474463   Recall:  0.9489267209474463\n",
      "15021-016750-DISCHARGE_SUMMARY.txt   Number:  1071   F1: 0.9738562091503268   Presicion:  0.9738562091503268   Recall:  0.9738562091503268\n",
      "12618-027862-DISCHARGE_SUMMARY.txt   Number:  1603   F1: 0.9494697442295695   Presicion:  0.9494697442295695   Recall:  0.9494697442295695\n",
      "07797-005646-DISCHARGE_SUMMARY.txt   Number:  1656   F1: 0.9547101449275363   Presicion:  0.9547101449275363   Recall:  0.9547101449275363\n",
      "21115-101632-ECHO_REPORT.txt   Number:  464   F1: 0.9181034482758621   Presicion:  0.9181034482758621   Recall:  0.9181034482758621\n",
      "10434-169426-RADIOLOGY_REPORT.txt   Number:  216   F1: 0.9861111111111112   Presicion:  0.9861111111111112   Recall:  0.9861111111111112\n",
      "19778-001791-DISCHARGE_SUMMARY.txt   Number:  1488   F1: 0.9227150537634409   Presicion:  0.9227150537634409   Recall:  0.9227150537634409\n",
      "\n",
      "Total number of words:  108856\n",
      "Final F1:  0.9529929448078195  Final Precision:  0.9529929448078195  Final Recall:  0.9529929448078195\n"
     ]
    }
   ],
   "source": [
    "all_label = []\n",
    "all_prediction = []\n",
    "for root, dirs, files in os.walk('data/output_disorder_detector/test_bio/'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file = 'data/output_disorder_detector/test_bio/' + file\n",
    "            f1, label, prediction = evaluate_with_single_file(file)\n",
    "            all_label = all_label + label\n",
    "            all_prediction = all_prediction + prediction\n",
    "\n",
    "print()\n",
    "print('Total number of words: ', len(all_prediction))\n",
    "print('Final F1: ',f1_score(all_label,all_prediction, average='micro'), ' Final Precision: ', precision_score(all_label,all_prediction,average='micro'), ' Final Recall: ', recall_score(all_label,all_prediction, average='micro')  )\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import using_split_current, preprocess_text, get_files, process_tag_file_multiple, process_tag_file_single_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results(self, res):\n",
    "        if len(res) < 2:\n",
    "            return res\n",
    "        else:\n",
    "            results = [res[0]]\n",
    "            for i in range(len(res)-1):\n",
    "                last_item = results[-1]\n",
    "                cur_item = res[i+1]\n",
    "\n",
    "                if cur_item[0] - last_item[1] == 1:\n",
    "                    results[-1] = (last_item[0],cur_item[1])\n",
    "                else:\n",
    "                    results.append(cur_item)\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "        results = []\n",
    "        \n",
    "        text = preprocess_text(text)\n",
    "        tokens = using_split_current(text)\n",
    "        \n",
    "        raw_text = [i[0] for i in tokens]\n",
    "        Y_tst = [(i[1],i[2]) for i in tokens]\n",
    "        Y_tst = np.array(Y_tst)\n",
    "        \n",
    "        X_tst = self.context(raw_text, size=20)\n",
    "        X_tst = np.array(X_tst)\n",
    "        \n",
    "        instances_vector = self.convert_to_vector(X_tst)\n",
    "        preds = self.model.predict(instances_vector)\n",
    "        out_index_preds = np.array([np.argmax(instance) for instance in preds])\n",
    "        results = np.where(out_index_preds == 1)\n",
    "\n",
    "        return self.convert_results(Y_tst[results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"|||| I can do it, I can finish it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = using_split_current(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llll', 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it']\n",
      "[(0, 4), (5, 6), (7, 10), (11, 13), (14, 16), (16, 17), (18, 19), (20, 23), (24, 30), (31, 33)]\n",
      "[[ 0  4]\n",
      " [ 5  6]\n",
      " [ 7 10]\n",
      " [11 13]\n",
      " [14 16]\n",
      " [16 17]\n",
      " [18 19]\n",
      " [20 23]\n",
      " [24 30]\n",
      " [31 33]]\n"
     ]
    }
   ],
   "source": [
    "raw_text = [i[0] for i in tokens]\n",
    "print(raw_text)\n",
    "Y_tst = [(i[1],i[2]) for i in tokens]\n",
    "print(Y_tst)\n",
    "Y_tst = np.array(Y_tst)\n",
    "print(Y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 'i', 'can', 'do', 'it', 'ª', 'i', 'can', 'finish', 'it', 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_tst = context(raw_text, size=20)\n",
    "print(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on '00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ALLREPORTS2/00176-102920-ECHO_REPORT.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(120, 140), (614, 642), (809, 851), (883, 894), (915, 926), (934, 942), (973, 984), (1201, 1210), (1290, 1296), (1297, 1318), (1413, 1434), (1482, 1502), (1729, 1757), (1892, 1917), (1947, 1958), (2218, 2220), (2363, 2384), (2417, 2428), (2443, 2451), (2659, 2679)]\n"
     ]
    }
   ],
   "source": [
    "with open(filename) as ff:\n",
    "    text = ff.read()\n",
    "#print(text)\n",
    "text = preprocess_text(text)\n",
    "tokens = using_split_current(text)   # [('102920', 0, 6), ('llll', 7, 11), ('176', 12, 15),...] \n",
    "\n",
    "raw_text = [i[0] for i in tokens]  # ['102920', 'llll', '176',...]\n",
    "Y_tst = [(i[1],i[2]) for i in tokens] #[(0, 6), (7, 11),...]\n",
    "Y_tst = np.array(Y_tst)  # #[[0, 6], [7, 11],...]\n",
    "\n",
    "text_slices = list_of_groups(raw_text, MAX_SEQUENCE_LENGTH)\n",
    "new_text_slices = [' '.join(i) for i in text_slices]\n",
    "\n",
    "X = t.texts_to_sequences(new_text_slices)\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "PREDICTION = model.predict([X, s0, c0])\n",
    "\n",
    "PREDICTION = np.asarray(PREDICTION)\n",
    "PREDICTION = PREDICTION.swapaxes(0,1)\n",
    "\n",
    "PR = np.argmax(PREDICTION, axis = -1)\n",
    "\n",
    "oneline_prediction = PR.flatten()\n",
    "oneline_prediction = oneline_prediction[:len(raw_text)]\n",
    "\n",
    "result_1 = np.where(oneline_prediction == 1)\n",
    "result_extend = extend(oneline_prediction, result_1)\n",
    "final_result = get_index(tokens, result_extend)\n",
    "print(final_result)\n",
    "\n",
    "with open('task1_output/'+filename.split('/')[-1],'w') as writer:\n",
    "            for item in final_result:\n",
    "                writer.write('%s||Disease_Disorder||CUI_CODE||%s||%s\\n' % (filename.split('/')[-1], item[0], item[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(prediction, result_1):\n",
    "    result = []\n",
    "    #result_1 = list(result_1)\n",
    "    for var in result_1[0]:\n",
    "        result_for_one_item = []\n",
    "        #print('*',var)\n",
    "        index = var\n",
    "        result_for_one_item.append(index)\n",
    "        while prediction[index+1] == 2:\n",
    "            result_for_one_item.append(index+1)\n",
    "            index += 1\n",
    "        result.append(result_for_one_item)  \n",
    "    return result\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(tokens, index_list):\n",
    "    final_result = []\n",
    "    for var in index_list:\n",
    "        #print('var: ',var)\n",
    "        begin_index = tokens[var[0]][1]\n",
    "        #print('begin_index', begin_index)\n",
    "        end_index = tokens[var[-1]][2]\n",
    "        #print('end_index', end_index)\n",
    "        final_result.append((begin_index, end_index))\n",
    "    #print(final_result)\n",
    "    return final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
